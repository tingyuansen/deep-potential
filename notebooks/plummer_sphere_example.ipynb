{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "YGF-i1bkt0We",
    "outputId": "df9b18f8-32fb-435c-d12d-adc16e720797"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "print(f'Pytorch version {torch.__version__}')\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.ticker import AutoMinorLocator, MultipleLocator\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from time import time\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "D8lJqCYit9UN",
    "outputId": "38155f1a-e6b4-4061-d8a4-5f56d1e2d5e2"
   },
   "outputs": [],
   "source": [
    "sys.path.append('../scripts/')\n",
    "import potential_torch\n",
    "import flow_torch\n",
    "import toy_systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8ASGsPPN_m5"
   },
   "source": [
    "# Theory\n",
    "\n",
    "## Plummer Sphere\n",
    "\n",
    "The Plummer sphere is a spherically symmetric, self-gravitating system. It has density\n",
    "$$\n",
    "  \\rho \\left( r \\right) = \\frac{3}{4 \\pi} \\left( 1 + r^2 \\right)^{-\\frac{5}{2}} \\, ,\n",
    "$$\n",
    "which generates the gravitational potential\n",
    "$$\n",
    "  \\Phi \\left( r \\right) = \\left( 1 + r^2 \\right)^{-\\frac{1}{2}} \\, .\n",
    "$$\n",
    "There exists a simple distribution function with isotropic velocities that renders the system stationary. Begin with the specific energy of an individual particle:\n",
    "$$\n",
    "  E = \\frac{1}{2} v^2 + \\Phi \\, .\n",
    "$$\n",
    "Then, the distribution function\n",
    "$$\n",
    "  f \\left( E \\right) \\propto\n",
    "  \\begin{cases}\n",
    "    \\left( -E \\right)^{\\frac{7}{2}} \\, , & E < 0 \\\\\n",
    "    0 \\, , & E > 0\n",
    "  \\end{cases}\n",
    "$$\n",
    "reduces to the above density (after integrating over velocity) and is stationary, since it only depends on a conserved quantity: the energy. Note that because the energy is always negative, all the particles are on bound orbits.\n",
    "$$\n",
    "  p \\left( E \\right) \\\\\n",
    "  p \\left( E \\mid r \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "  p \\left( r \\right) \\propto r^2 \\rho \\left( r \\right) \\\\\n",
    "  p \\left( v \\mid r \\right) \\propto v^2 \\left[ \\left( 1 + r^2 \\right)^{-\\frac{1}{2}} + \\frac{1}{2} v^2 \\right]^{\\frac{7}{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xeammv_nio5V"
   },
   "source": [
    "## Verify stationarity in ideal case\n",
    "\n",
    "We can verify that the combination of the above distribution function and potential renders the system stationary. Stationarity requires that\n",
    "$$\n",
    "  \\frac{\\partial f}{\\partial t}\n",
    "  = \\left\\{ \\mathcal{H} , f \\right\\}\n",
    "  = \\sum_i \\left(\n",
    "      \\frac{\\partial \\Phi}{\\partial x_i} \\frac{\\partial f}{\\partial v_i}\n",
    "    - v_i \\frac{\\partial f}{\\partial x_i}\n",
    "    \\right)\n",
    "  = 0 \\, .\n",
    "$$\n",
    "In order to empirically verify that this condition holds throughout phase space, we draw random points in phase space (not necessarily from the distribution function - the stationarity relation should hold everywhere in phase space), calculate $\\Phi$ and $f$, use pytorch to evaluate the gradients, and finally calculate the above Poisson bracket. It should be zero at every point that we draw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.from_numpy(np.random.normal(size=[10,3])), requires_grad=True)\n",
    "v = Variable(torch.from_numpy(np.random.normal(size=[10,3])), requires_grad=True)\n",
    "\n",
    "r2 = torch.sum(x**2, axis=1)\n",
    "v2 = torch.sum(v**2, axis=1)\n",
    "\n",
    "Phi = (-(1+r2)**(-1/2))\n",
    "E = v2/2 + Phi\n",
    "\n",
    "f = (torch.clamp(-E, 0, np.inf)**(7/2))\n",
    "\n",
    "df_dv = torch.autograd.grad(f, v,\\\n",
    "                            grad_outputs=torch.ones_like(f), retain_graph=True,\\\n",
    "                            create_graph=True)[0]\n",
    "df_dx = torch.autograd.grad(f, x,\\\n",
    "                            grad_outputs=torch.ones_like(f), retain_graph=True,\\\n",
    "                            create_graph=True)[0]\n",
    "dPhi_dx = torch.autograd.grad(Phi, x,\\\n",
    "                            grad_outputs=torch.ones_like(Phi), retain_graph=True,\\\n",
    "                            create_graph=True)[0]\n",
    "\n",
    "df_dt = torch.sum(dPhi_dx * df_dv - df_dx * v, axis=1)\n",
    "print(f'{{H,f}} = {df_dt}')\n",
    "\n",
    "np.testing.assert_allclose(df_dt.detach().numpy(), 0., atol=1.e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlV7fI11lplA"
   },
   "source": [
    "## Mock data\n",
    "\n",
    "We now draw phase-space points from the Plummer sphere distribution function. This will be used as input data to train a normalizing flow representing the distribution function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJXrdgAjD263"
   },
   "outputs": [],
   "source": [
    "# Instantiate Plummer sphere class\n",
    "plummer_sphere = toy_systems.PlummerSphere()\n",
    "\n",
    "def sample_df(n_samples, max_dist=None):\n",
    "    \"\"\"\n",
    "    Returns phase-space locations sampled from the Plummer sphere\n",
    "    distribution function. The shape of the output is\n",
    "    (n_samples, 6).\n",
    "    \"\"\"\n",
    "    x,v = plummer_sphere.sample_df(n_samples)\n",
    "    if max_dist is not None:\n",
    "        r2 = np.sum(x**2, axis=1)\n",
    "        idx = (r2 < max_dist**2)\n",
    "        x = x[idx]\n",
    "        v = v[idx]\n",
    "    \n",
    "    return torch.cat([torch.Tensor(x.astype('f4')), torch.Tensor(v.astype('f4'))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tdxFA81fuOPs"
   },
   "outputs": [],
   "source": [
    "n_samples = 1024 * 128\n",
    "data = sample_df(n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iAnm5FFmItj0"
   },
   "source": [
    "We plot the mock data in a few different projections of phase space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcylOyZvLqvn"
   },
   "outputs": [],
   "source": [
    "def vec2ang(x):\n",
    "    phi = np.arctan2(x[:,1], x[:,0])\n",
    "    theta = np.arctan2(x[:,2], np.sqrt(x[:,0]**2+x[:,1]**2))\n",
    "    return theta, phi\n",
    "\n",
    "def plot_samples(eta):\n",
    "    fig,ax_arr = plt.subplots(\n",
    "        3,3,\n",
    "        figsize=(13,12),\n",
    "        subplot_kw=dict(aspect='equal')\n",
    "    )\n",
    "    fig.subplots_adjust(wspace=0.30, hspace=0.25)\n",
    "\n",
    "    xlim = (-3., 3.)\n",
    "    vlim = (-1.5, 1.5)\n",
    "\n",
    "    for k,(i,j) in enumerate([(0,1), (0,2), (1,2)]):\n",
    "        ax_arr[0,k].hist2d(eta[:,i], eta[:,j], bins=31, range=[xlim,xlim])\n",
    "        ax_arr[1,k].hist2d(eta[:,i+3], eta[:,j+3], bins=31, range=[vlim,vlim])\n",
    "\n",
    "        ax_arr[0,k].set_xlabel(rf'$x_{i}$')\n",
    "        ax_arr[0,k].set_ylabel(rf'$x_{j}$', labelpad=-5)\n",
    "        ax_arr[1,k].set_xlabel(rf'$v_{i}$')\n",
    "        ax_arr[1,k].set_ylabel(rf'$v_{j}$', labelpad=-5)\n",
    "    r = np.sqrt(np.sum(eta[:,:3]**2, axis=1))\n",
    "    v = np.sqrt(np.sum(eta[:,3:]**2, axis=1))\n",
    "    ax_arr[2,0].hist2d(r, v, bins=31, range=[(0.,5.),(0.,1.5)])\n",
    "    ax_arr[2,0].set_xlabel(r'$r$')\n",
    "    ax_arr[2,0].set_ylabel(r'$v$', labelpad=0)\n",
    "    \n",
    "    bins = 11\n",
    "    v0 = eta.shape[0] / bins**2\n",
    "    dv = 0.5*v0\n",
    "\n",
    "    theta, phi = vec2ang(eta[:,:3])\n",
    "    ax_arr[2,1].hist2d(\n",
    "        phi, np.sin(theta),\n",
    "        bins=bins,\n",
    "        vmin=v0-dv, vmax=v0+dv,\n",
    "        cmap='bwr_r'\n",
    "    )\n",
    "    ax_arr[2,1].set_xlabel(r'$\\varphi_x$')\n",
    "    ax_arr[2,1].set_ylabel(r'$\\sin \\theta_x$', labelpad=-5)\n",
    "\n",
    "    theta, phi = vec2ang(eta[:,3:])\n",
    "    ax_arr[2,2].hist2d(\n",
    "        phi, np.sin(theta),\n",
    "        bins=bins,\n",
    "        vmin=v0-dv, vmax=v0+dv,\n",
    "        cmap='bwr_r'\n",
    "    )\n",
    "    ax_arr[2,2].set_xlabel(r'$\\varphi_v$')\n",
    "    ax_arr[2,2].set_ylabel(r'$\\sin \\theta_v$', labelpad=-5)\n",
    "\n",
    "    for a in ax_arr[2]:\n",
    "        a.set_aspect('auto')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 720
    },
    "colab_type": "code",
    "id": "o82idmgSNfDo",
    "outputId": "c3ceb682-0d83-40ae-faa9-76137c0a6252"
   },
   "outputs": [],
   "source": [
    "fig = plot_samples(data.numpy())\n",
    "fig.savefig('plummer_df_samples.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "22W1xJiOu6Tw"
   },
   "source": [
    "Above, the first row of panels shows the spatial density of points, with one dimension projected out in each column. The second row shows the density of points in velocity space, projecting out one dimension in each column. The left panel of the third row shows the distribution of points in radius and speed. The middle panel of the last row shows how isotropically the points are distributed in space around the origin, while the right panel of the last row shows how isotropically points are distributed in velocity. As the number of samples goes to infinity, both panels should become white (perfect isotropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZEKMrjLpYuh"
   },
   "source": [
    "We can also compare the histogram of samples to the theoretical distribution function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DxPf6wVb1r34",
    "outputId": "d9a1899c-4642-4d18-9a03-fa93bb1efae8"
   },
   "outputs": [],
   "source": [
    "r_lim = (0., 5.)\n",
    "v_lim = (0., 1.5)\n",
    "bins = (50, 50)\n",
    "\n",
    "r = np.linspace(r_lim[0], r_lim[1], bins[0]+1)\n",
    "v = np.linspace(v_lim[0], v_lim[1], bins[1]+1)\n",
    "\n",
    "r = 0.5 * (r[:-1] + r[1:])\n",
    "v = 0.5 * (v[:-1] + v[1:])\n",
    "\n",
    "rr,vv = np.meshgrid(r, v)\n",
    "\n",
    "psi = 1. / np.sqrt(1+rr**2)\n",
    "E = psi - vv**2 / 2\n",
    "df = np.clip(E, 0., np.inf)**(7/2)\n",
    "A = 24 * np.sqrt(2.) / (7 * np.pi**3)\n",
    "\n",
    "n = A * (4*np.pi)**2 * rr**2 * vv**2 * df\n",
    "\n",
    "fig,ax_arr = plt.subplots(3,2, figsize=(11,16))\n",
    "fig.subplots_adjust(left=0.1)\n",
    "\n",
    "# Ideal distribution\n",
    "ax_arr[0,0].imshow(\n",
    "    n,\n",
    "    extent=r_lim+v_lim,\n",
    "    origin='lower',\n",
    "    aspect='auto',\n",
    "    interpolation='nearest'\n",
    ")\n",
    "\n",
    "img = np.log(n)\n",
    "vmax = np.max(img)\n",
    "ax_arr[0,1].imshow(\n",
    "    img,\n",
    "    extent=r_lim+v_lim,\n",
    "    vmax=vmax,\n",
    "    vmin=vmax-10.,\n",
    "    origin='lower',\n",
    "    aspect='auto',\n",
    "    interpolation='nearest'\n",
    ")\n",
    "\n",
    "dr = r[1] - r[0]\n",
    "dv = v[1] - v[0]\n",
    "N = np.sum(n) * dr * dv\n",
    "print(f'\\int f(x,v) d^3x d^3v = {N:.5f}')\n",
    "\n",
    "# 2D histogram of samples\n",
    "n_samples = 1024*1024*4\n",
    "plummer_sphere = toy_systems.PlummerSphere()\n",
    "x_samp,v_samp = plummer_sphere.sample_df(n_samples)\n",
    "r_samp = np.sqrt(np.sum(x_samp**2, axis=1))\n",
    "v_samp = np.sqrt(np.sum(v_samp**2, axis=1))\n",
    "\n",
    "n_samp,_,_,_ = ax_arr[1,0].hist2d(r_samp, v_samp, bins=bins, range=[r_lim,v_lim])\n",
    "n_samp = n_samp.T\n",
    "\n",
    "ax_arr[1,1].hist2d(\n",
    "    r_samp, v_samp,\n",
    "    bins=bins,\n",
    "    range=[r_lim,v_lim],\n",
    "    norm=mcolors.LogNorm()\n",
    ")\n",
    "\n",
    "# Residuals (samples - ideal)\n",
    "n_0 = n*dr*dv * n_samples\n",
    "img = (n_samp - n_0) / n_0\n",
    "ax_arr[2,0].imshow(\n",
    "    img,\n",
    "    extent=r_lim+v_lim,\n",
    "    vmax=0.1,\n",
    "    vmin=-0.1,\n",
    "    origin='lower',\n",
    "    aspect='auto',\n",
    "    cmap='coolwarm_r',\n",
    "    interpolation='nearest'\n",
    ")\n",
    "\n",
    "ax_arr[2,1].imshow(\n",
    "    np.log(n_samp) - np.log(n_0),\n",
    "    extent=r_lim+v_lim,\n",
    "    vmax=1.,\n",
    "    vmin=-1.,\n",
    "    origin='lower',\n",
    "    aspect='auto',\n",
    "    cmap='coolwarm_r',\n",
    "    interpolation='nearest'\n",
    ")\n",
    "\n",
    "# Zero-energy line\n",
    "for a in ax_arr.flat:\n",
    "    a.plot(r, np.sqrt(2.) * (1+r**2)**(-1/4), c='r')\n",
    "    a.set_xlabel(r'$r$')\n",
    "    a.set_ylabel(r'$v$')\n",
    "    a.text(\n",
    "        0.95, 0.95, r'$E > 0$',\n",
    "        ha='right', va='top',\n",
    "        fontsize=16, c='r',\n",
    "        transform=a.transAxes\n",
    "    )\n",
    "\n",
    "# Labels\n",
    "pos = ax_arr[0,0].get_position()\n",
    "y_txt = 0.5 * (pos.y0 + pos.y1)\n",
    "fig.text(\n",
    "    0.02, y_txt,\n",
    "    r'Ideal DF',\n",
    "    rotation=90.,\n",
    "    ha='left',\n",
    "    va='center',\n",
    "    fontsize=18,\n",
    ")\n",
    "\n",
    "pos = ax_arr[1,0].get_position()\n",
    "y_txt = 0.5 * (pos.y0 + pos.y1)\n",
    "fig.text(\n",
    "    0.02, y_txt,\n",
    "    r'Samples from DF',\n",
    "    rotation=90.,\n",
    "    ha='left',\n",
    "    va='center',\n",
    "    fontsize=18,\n",
    ")\n",
    "\n",
    "pos = ax_arr[2,0].get_position()\n",
    "y_txt = 0.5 * (pos.y0 + pos.y1)\n",
    "fig.text(\n",
    "    0.02, y_txt,\n",
    "    r'Residuals (Samples - Ideal)',\n",
    "    rotation=90.,\n",
    "    ha='left',\n",
    "    va='center',\n",
    "    fontsize=18,\n",
    ")\n",
    "\n",
    "ax_arr[0,0].set_title('Linear Scale', fontsize=18)\n",
    "ax_arr[0,1].set_title('Log Scale', fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "96mLTFTpr6Oq"
   },
   "source": [
    "## Fit potential using ideal distribution function\n",
    "\n",
    "We now verify that we recover the correct potential when we use the analytic distribution function (rather than a normalizing flow approximation). We use a model of the potential with a single free variable: the amplitude of the potential. The shape of the potential is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gq-W0vcxHoGc"
   },
   "outputs": [],
   "source": [
    "a = Variable(torch.Tensor([1.0]), requires_grad=True) # The correct value would be a = -1.\n",
    "\n",
    "def phi_analytic(q):\n",
    "    q2 = torch.sum(q**2, axis=1)\n",
    "    return a / torch.sqrt(1 + q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j2RVhAeYIKQX"
   },
   "source": [
    "The true distribution function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RiR6fyePsJwL"
   },
   "outputs": [],
   "source": [
    "def df_ideal(q, p):\n",
    "    r2 = torch.sum(q**2, axis=1)\n",
    "    v2 = torch.sum(p**2, axis=1)\n",
    "\n",
    "    Phi = -(1+r2)**(-1/2)\n",
    "    E = v2/2 + Phi\n",
    "\n",
    "    f = torch.clamp(-E, 0, np.inf)**(7/2)\n",
    "\n",
    "    A = 24 * np.sqrt(2.) / (7. * np.pi**3)\n",
    "\n",
    "    return A * f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4U90u_MfIOIK"
   },
   "source": [
    "We calculate the gradients of the distribution function first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRB4HgXu2lg6"
   },
   "outputs": [],
   "source": [
    "n_points = 1024 * 128\n",
    "\n",
    "q,p = plummer_sphere.sample_df(n_points)\n",
    "q = Variable(torch.from_numpy(q.astype('f4')), requires_grad=True)\n",
    "p = Variable(torch.from_numpy(p.astype('f4')), requires_grad=True)\n",
    "\n",
    "f = df_ideal(q, p)\n",
    "\n",
    "df_dq = torch.autograd.grad(f, q,\\\n",
    "                            grad_outputs=torch.ones_like(f), retain_graph=True,\\\n",
    "                            create_graph=True)[0]\n",
    "df_dp = torch.autograd.grad(f, p,\\\n",
    "                            grad_outputs=torch.ones_like(f), retain_graph=True,\\\n",
    "                            create_graph=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qeJ2D7DN2oO9"
   },
   "source": [
    "Then, we plot the loss as a function of the amplitude $a$ of the potential. We add in different amounts of noise to the gradients of the distribution function, and see how that biases the best-fit value of $a$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "colab_type": "code",
    "id": "APy80sbY7EeT",
    "outputId": "e22e3d21-2c49-43bb-d6ea-bfb65645cb2c"
   },
   "outputs": [],
   "source": [
    "a_range = torch.Tensor(np.arange(-1.4, -0.6, 0.01))\n",
    "\n",
    "fig,ax = plt.subplots(1,1, figsize=(8,6), dpi=100)\n",
    "\n",
    "lam = torch.Tensor([1.0])  # How much to penalize negative matter densities\n",
    "mu = torch.Tensor([0.0])   # How much to penalize positive matter densities\n",
    "\n",
    "phi_param = [a]\n",
    "\n",
    "for err in [0., 0.001, 0.004, 0.016]:\n",
    "    loss_range = []\n",
    "\n",
    "    for aa in a_range:\n",
    "        a.data = aa\n",
    "        df_dq_est = df_dq + torch.normal(torch.zeros(df_dq.shape), torch.ones(df_dq.shape)*err)\n",
    "        df_dp_est = df_dp + torch.normal(torch.zeros(df_dp.shape), torch.ones(df_dp.shape)*err)\n",
    "\n",
    "        loss, dloss_dparam = potential_torch.get_phi_loss_gradients(\n",
    "            phi_analytic, phi_param,\n",
    "            q, p,\n",
    "            df_dq=df_dq_est,\n",
    "            df_dp=df_dp_est,\n",
    "            weight_samples=False,\n",
    "            lam=lam,\n",
    "            mu=mu\n",
    "        )\n",
    "\n",
    "        loss_range.append(loss.detach().numpy())\n",
    "\n",
    "    loss_range = np.array(loss_range)\n",
    "    loss_range -= np.min(loss_range)\n",
    "    ax.plot(a_range, loss_range, label=f'$\\sigma = {err}$')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel(r'loss')\n",
    "ax.set_xlabel(r'$a$')\n",
    "ax.set_title(r'$\\Phi \\left( r \\right) = a \\left( 1 + r^2 \\right)^{-1/2}$')\n",
    "\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "ax.grid('on', which='major', alpha=0.2)\n",
    "ax.grid('on', which='minor', alpha=0.05)\n",
    "\n",
    "fig.savefig('phi_bias.png', dpi=120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RgYM6t2Q32Lu"
   },
   "source": [
    "As the noise in $\\frac{\\partial f}{\\partial \\vec{\\eta}}$ increases, the estimate of the amplitude $a$ of the potential becomes increasingly biased, in that it favors shallower potentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JC562LO-srf2"
   },
   "source": [
    "## Train ensemble of normalizing flows to represent the distribution function\n",
    "\n",
    "We now train an ensemble of normalizing flows to approximate the distribution function, $f \\left( \\vec{\\eta} \\right)$. We first draw phase-space points from the true distribution function. Then, we train multiple normalizing flows on the same set of points.\n",
    "\n",
    "The reason for training multiple normalizing flows, rather than just one, is to average down errors in the individual normalizing flows. In the end, we want to predict $\\frac{\\partial f}{\\partial \\vec{\\eta}}$ as accurately as possible, because inaccurate estimates can bias our inferred gravitational potential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4460Pd0yjG_J"
   },
   "source": [
    "Draw samples from the true distribution function. Filter out samples at $r > 10$, because these points produce massive gradients in the training procedure. Later on, we should deal with large gradients in a more automatic way (e.g., gradient clipping):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVrX7jCZyx33"
   },
   "outputs": [],
   "source": [
    "n_samples = 1024 * 128\n",
    "data = sample_df(int(1.2 * n_samples), max_dist=10.0)\n",
    "data = torch.Tensor(data[:n_samples,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OdpfbKxdjZp5"
   },
   "source": [
    "Train multiple normalizing flows on the same dataset. Save each normalizing flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_flows = 100\n",
    "\n",
    "n_dim = 6\n",
    "n_units = 4\n",
    "\n",
    "n_epochs = 32\n",
    "batch_size = 1024\n",
    "\n",
    "n_steps = n_samples * n_epochs // batch_size\n",
    "print(f'n_steps = {n_steps}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "St2w_oxQHZTA"
   },
   "outputs": [],
   "source": [
    "# for i in range(n_flows):\n",
    "#     print(f'Training flow {i+1} of {n_flows} ...')\n",
    "#     flow = flow_torch.NormalizingFlow(n_dim, n_units)\n",
    "#     loss_history = flow_torch.train_flow(\n",
    "#         flow, data, \n",
    "#         n_epochs=n_epochs,\n",
    "#         batch_size=batch_size,\n",
    "#         callback=flow_torch.get_training_callback(\n",
    "#             flow,\n",
    "#             plt_fn=None,\n",
    "#             every=1024, \n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     torch.save(flow.state_dict(), f'plummer_flow/plummer_flow_{i:02d}.pth')\n",
    "       \n",
    "#     x_sample = flow.dist.sample(sample_shape=[128*1024])\n",
    "#     y_sample, _ = flow.backward(x_sample)\n",
    "#     fig = plot_samples(y_sample.detach().numpy())\n",
    "#     fig.savefig(f'plummer_flow_{i:02d}.png', dpi=100)\n",
    "#     plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OnogOiU4--pE"
   },
   "source": [
    "### Test the accuracy of the gradients\n",
    "\n",
    "First, draw a set of points $\\vec{\\eta}$ from the true distribution fuction $f$, and compute the gradient of the true distribution w.r.t. $\\vec{\\eta}$ at each point:\n",
    "$$\n",
    "  \\frac{\\partial f}{\\partial \\vec{\\eta}} \\, .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-HbYmXH09o3s"
   },
   "outputs": [],
   "source": [
    "n_points = 1024 * 32\n",
    "\n",
    "q,p = plummer_sphere.sample_df(n_points)\n",
    "q = Variable(torch.Tensor(q.astype('f4')), requires_grad=True)\n",
    "p = Variable(torch.Tensor(p.astype('f4')), requires_grad=True)\n",
    "\n",
    "f_ideal, df_dq, df_dp = potential_torch.calc_df_deta(df_ideal, q, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RInbYnbpctuv"
   },
   "source": [
    "Then, compute the gradients of the best-fit approximation to the distribution function $f^{\\ast}$ w.r.t. $\\vec{\\eta}$ at each point:\n",
    "$$\n",
    "  \\frac{\\partial f^{\\ast}}{\\partial \\vec{\\eta}} \\, .\n",
    "$$\n",
    "We calculate the gradients using each normalizing flow in our ensemble, and take the mean of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "id": "-lBA16g_wIGD",
    "outputId": "c50e4d66-873a-4c8d-d8e5-4e1682a04540"
   },
   "outputs": [],
   "source": [
    "eta = torch.cat([q,p], axis=1)\n",
    "\n",
    "f_star_list, dflow_dq_list, dflow_dp_list = [], [], []\n",
    "f_star = np.zeros_like(f_ideal.detach().numpy())\n",
    "dflow_dq = np.zeros_like(df_dq.detach().numpy())\n",
    "dflow_dp = np.zeros_like(df_dp.detach().numpy())\n",
    "\n",
    "n_flows = 100\n",
    "\n",
    "for i in range(n_flows):\n",
    "    print(f'Loading flow {i+1} of {n_flows} ...')\n",
    "    \n",
    "    fname = f'../plummer_flow/plummer_flow_{i:02d}.pth'\n",
    "    state_dict = torch.load(fname)\n",
    "    flow = flow_torch.NormalizingFlow(n_dim, n_units)\n",
    "    flow.load_state_dict(state_dict)\n",
    "\n",
    "    def get_f_star(q, p):\n",
    "        eta = torch.cat([q,p], axis=1)\n",
    "        _ , prior_logprob, log_det = flow(eta)\n",
    "        c = prior_logprob + log_det\n",
    "        return torch.exp(c)\n",
    "    \n",
    "    res = potential_torch.calc_df_deta(get_f_star, q, p)\n",
    "    f_star_list.append(res[0].detach().numpy())\n",
    "    dflow_dq_list.append(res[1].detach().numpy())\n",
    "    dflow_dp_list.append(res[2].detach().numpy())\n",
    "\n",
    "    f_star += res[0].detach().numpy() / n_flows\n",
    "    dflow_dq += res[1].detach().numpy() / n_flows\n",
    "    dflow_dp += res[2].detach().numpy() / n_flows\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez(\"training_gradients_nflows=9600.npz\",\\\n",
    "#          df_dq = df_dq.detach().numpy(),\n",
    "#          df_dp = df_dp.detach().numpy(),\n",
    "#          q = q.detach().numpy(),\\\n",
    "#          p = p.detach().numpy(),\\\n",
    "#          f_ideal = f_ideal.detach().numpy(),\\\n",
    "#          f_star_list = f_star_list,\\\n",
    "#          dflow_dq_list = dflow_dq_list,\\\n",
    "#          dflow_dp_list = dflow_dp_list,\\\n",
    "#          f_star = f_star,\\\n",
    "#          dflow_dq = dflow_dq,\\\n",
    "#          dflow_dp = dflow_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = np.load(\"training_gradients_nflows=9600.npz\")\n",
    "# df_dq = Variable(torch.Tensor(temp[\"df_dq\"]), requires_grad=True)\n",
    "# df_dp = Variable(torch.Tensor(temp[\"df_dp\"]), requires_grad=True)\n",
    "# q = Variable(torch.Tensor(temp[\"q\"]), requires_grad=True)\n",
    "# p = Variable(torch.Tensor(temp[\"p\"]), requires_grad=True)\n",
    "# f_ideal = Variable(torch.Tensor(temp[\"f_ideal\"]), requires_grad=True)\n",
    "# f_star_list = list(temp[\"f_star_list\"])\n",
    "# dflow_dq_list = list(temp[\"dflow_dq_list\"])\n",
    "# dflow_dp_list = list(temp[\"dflow_dp_list\"])\n",
    "# f_star = temp[\"f_star\"]\n",
    "# dflow_dq = temp[\"dflow_dq\"]\n",
    "# dflow_dp = temp[\"dflow_dp\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3OTcjTdckPLb"
   },
   "source": [
    "Plot the true vs. estimated gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "colab_type": "code",
    "id": "10-Gjg7xr46k",
    "outputId": "933ef33e-22ab-4486-9ba2-58ff9190effd"
   },
   "outputs": [],
   "source": [
    "fig,ax_arr = plt.subplots(2,3, figsize=(16,9))\n",
    "\n",
    "def sigma_clipped_mean(x, n_sigma=3.):\n",
    "    sigma = np.std(x, axis=0)\n",
    "    mu = np.median(x, axis=0)\n",
    "    idx = np.abs(x - mu[None,...]) < n_sigma*(sigma[None,...]+1.e-8)\n",
    "    w = idx.astype(x.dtype)\n",
    "    x_avg = np.sum(x*w, axis=0) / np.sum(w, axis=0)\n",
    "    return x_avg\n",
    "\n",
    "df_dq_est = sigma_clipped_mean(np.stack(dflow_dq_list, axis=0), n_sigma=5)\n",
    "df_dp_est = sigma_clipped_mean(np.stack(dflow_dp_list, axis=0), n_sigma=5)\n",
    "\n",
    "for i,ax in enumerate(ax_arr.flat):\n",
    "    ax.set_aspect('equal')\n",
    "    if i < 3:\n",
    "        ax.scatter(\n",
    "            df_dq[:,i].detach(),\n",
    "            df_dq_est[:,i],\n",
    "            alpha=0.1, s=2,\n",
    "            edgecolors='none'\n",
    "        )\n",
    "    else:\n",
    "        ax.scatter(\n",
    "            df_dp[:,i-3].detach(),\n",
    "            df_dp_est[:,i-3],\n",
    "            alpha=0.1, s=2,\n",
    "            edgecolors='none'\n",
    "        )\n",
    "\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    xlim = (min(xlim[0], ylim[0]), max(xlim[1], ylim[1]))\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(xlim)\n",
    "\n",
    "    ax.plot([xlim[0],xlim[1]], [xlim[0],xlim[1]], c='k', alpha=0.25)\n",
    "\n",
    "    ax.set_xlabel(r'true')\n",
    "    ax.set_ylabel(r'normalizing flow')\n",
    "\n",
    "    ax.set_title(rf'$\\mathrm{{d}}f / \\mathrm{{d}}\\eta_{i}$')\n",
    "\n",
    "fig.subplots_adjust(\n",
    "    hspace=0.25, wspace=0.3,\n",
    "    top=0.91, bottom=0.06\n",
    ")\n",
    "fig.suptitle('Performance of normalizing flow gradients', fontsize=20)\n",
    "\n",
    "fig.savefig('flow_gradients_comparison.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bKds0EaikSfU"
   },
   "source": [
    "Plot a histogram of the gradient residuals along each dimension in phase space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "colab_type": "code",
    "id": "KhjWFH-65SlT",
    "outputId": "2711269f-14e0-4618-ce0a-9cb2a154e029"
   },
   "outputs": [],
   "source": [
    "fig,ax_arr = plt.subplots(2,3, figsize=(16,9))\n",
    "\n",
    "for i,ax in enumerate(ax_arr.flat):\n",
    "    ax.set_aspect('auto')\n",
    "    if i < 3:\n",
    "        resid = df_dq_est[:,i] - df_dq[:,i].detach().numpy()\n",
    "    else:\n",
    "        resid = df_dp_est[:,i-3] - df_dp[:,i-3].detach().numpy()\n",
    "    \n",
    "    ax.hist(\n",
    "        resid,\n",
    "        range=(-0.05, 0.05),\n",
    "        bins=51,\n",
    "        log=True\n",
    "    )\n",
    "    ax.set_xlabel(r'(normalizing flow) - (true)')\n",
    "    ax.set_title(rf'$\\mathrm{{d}}f / \\mathrm{{d}}\\eta_{i}$')\n",
    "\n",
    "    sigma = np.std(resid)\n",
    "    kurt = scipy.stats.kurtosis(resid)\n",
    "    ax.text(\n",
    "        0.95, 0.95,\n",
    "        rf'$\\sigma = {sigma:.4f}$'+'\\n'+rf'$\\kappa = {kurt:.2f}$',\n",
    "        ha='right',\n",
    "        va='top',\n",
    "        transform=ax.transAxes\n",
    "    )\n",
    "\n",
    "fig.subplots_adjust(\n",
    "    hspace=0.25, wspace=0.3,\n",
    "    top=0.91, bottom=0.06\n",
    ")\n",
    "fig.suptitle('Performance of normalizing flow gradients', fontsize=20)\n",
    "\n",
    "fig.savefig('flow_gradients_comparison_hist.png', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uTR2vYKd_c84"
   },
   "source": [
    "### Fitting one-parameter model using estimated gradients of DF\n",
    "\n",
    "We now fit the simple, one-parameter analytic model of the potential, using the gradients estimated using our normalizing flows. We fit the model using the gradients calculated from the individual flows, and also using the ensemble of flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "colab_type": "code",
    "id": "7bw3AuUiI826",
    "outputId": "9c2fb584-7410-4bd1-c77f-08cdbfa5ca8e"
   },
   "outputs": [],
   "source": [
    "a_range = torch.Tensor(np.arange(-1.2, -0.2, 0.01))\n",
    "\n",
    "fig,ax = plt.subplots(1,1, figsize=(8,6), dpi=100)\n",
    "\n",
    "lam = torch.Tensor([1.0])\n",
    "mu = torch.Tensor([0.0])\n",
    "    \n",
    "phi_param = [a]\n",
    "\n",
    "for i in range(100+1):\n",
    "#for i in range(n_flows+1):\n",
    "    loss_range = []\n",
    "    \n",
    "    if i < 100:\n",
    "    #if i < n_flows:\n",
    "        df_dq_i = dflow_dq_list[i]\n",
    "        df_dp_i = dflow_dp_list[i]\n",
    "        alpha = 0.1\n",
    "        label = f'individual flows' if i == 0 else None\n",
    "    else:\n",
    "        df_dq_i = dflow_dq\n",
    "        df_dp_i = dflow_dp\n",
    "        alpha = 1.0\n",
    "        label = 'mean'\n",
    "    \n",
    "    for aa in a_range:\n",
    "        a.data = aa\n",
    "\n",
    "        loss, dloss_dparam = potential_torch.get_phi_loss_gradients(\n",
    "            phi_analytic, phi_param,\n",
    "            q, p,\n",
    "            df_dq=torch.Tensor(df_dq_i),\n",
    "            df_dp=torch.Tensor(df_dp_i),\n",
    "            weight_samples=False,\n",
    "            lam=lam,\n",
    "            mu=mu\n",
    "        )\n",
    "\n",
    "        loss_range.append(loss.detach().numpy())\n",
    "\n",
    "    loss_range = np.array(loss_range)\n",
    "    loss_range -= np.min(loss_range)\n",
    "    ax.plot(a_range, loss_range, label=label, c='b', alpha=alpha)\n",
    "\n",
    "ax.set_ylabel(r'loss')\n",
    "ax.set_xlabel(r'$a$')\n",
    "ax.set_title(r'$\\Phi \\left( r \\right) = a \\left( 1 + r^2 \\right)^{-1/2}$')\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "ax.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "ax.grid('on', which='major', alpha=0.2)\n",
    "ax.grid('on', which='minor', alpha=0.05)\n",
    "\n",
    "fig.savefig('phi_bias.png', dpi=120)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYXtln0DCT8c"
   },
   "source": [
    "We can see that the result we get using the mean of the normalizing flows has a much smaller bias than the result we get using the individual normalizing flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMENvHXlHTml"
   },
   "source": [
    "## Generate training dataset using ensemble of flows\n",
    "\n",
    "Pack the data as an array of shape `(sample, X, 3)`, where $X \\in \\left\\{ q, p, \\frac{\\partial f}{\\partial q} , \\frac{\\partial f}{\\partial p} \\right\\}$. This data will be used to fit the gravitational potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kz8e9tvEHS4j"
   },
   "outputs": [],
   "source": [
    "n_data = q.shape[0]\n",
    "data = torch.stack([q, p, torch.Tensor(dflow_dq), torch.Tensor(dflow_dp)], axis=1)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    data = data.cuda()\n",
    "    lam = lam.cuda()\n",
    "    mu = mu.cuda()\n",
    "    \n",
    "batch_size = 1024\n",
    "train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size,\\\n",
    "                                           shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y35R0VmAxr21"
   },
   "source": [
    "# Fit full model of $\\Phi \\left( \\vec{q} \\right)$\n",
    "\n",
    "Finally, we fit a flexible model of the potential to the gradients estimated from the ensemble of normalizing flows. We will use a simple feed-forward neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "28zm_VDaDoEK",
    "outputId": "47275099-94e2-4226-b33d-f83939e784a5"
   },
   "outputs": [],
   "source": [
    "phi_model = potential_torch.PhiNN(n_dim=3, n_hidden=3, n_features=128)\n",
    "\n",
    "# load trained model\n",
    "if torch.cuda.is_available():\n",
    "    state_dict = torch.load('plummer_sphere_phi_lambda=2_mu=0.pth')\n",
    "else:\n",
    "    state_dict = torch.load('plummer_sphere_phi_lambda=2_mu=0.pth',\\\n",
    "                            map_location=torch.device('cpu'))\n",
    "phi_model.load_state_dict(state_dict)\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    phi_model = phi_model.cuda()\n",
    "\n",
    "phi_param = [p for p in phi_model.parameters() if p.requires_grad==True]\n",
    "n_variables = sum([int(np.prod(np.array(list(param.shape)))) for param in phi_param])\n",
    "print(f'{n_variables} variables in the gravitational potential model.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jhKIU2C2Cl4L"
   },
   "source": [
    "A function to plot the potential:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xYEa5K8NwUEX"
   },
   "outputs": [],
   "source": [
    "def plot_model(phi_model, x, v, q):\n",
    "        \n",
    "    Phi = phi_model(q)\n",
    "\n",
    "    r = np.sqrt(np.sum(x**2, axis=1))\n",
    "    Phi_ideal = plummer_sphere.phi(r)\n",
    "\n",
    "    Phi_0 = np.median(Phi_ideal - Phi.detach().cpu().numpy())\n",
    "    Phi += Phi_0\n",
    "\n",
    "    fig,ax = plt.subplots(1,1, figsize=(8,6))\n",
    "\n",
    "    r_range = np.linspace(0.05, 50., 1000)\n",
    "    ax.semilogx(\n",
    "        r_range,\n",
    "        plummer_sphere.phi(r_range),\n",
    "        c='g', alpha=0.2,\n",
    "        label='ideal'\n",
    "    )\n",
    "    ax.scatter(r, Phi.detach().cpu(), alpha=0.2, s=3, label='NN model')\n",
    "    ax.legend(loc='upper left')\n",
    "\n",
    "    ax.set_xlim(0.05, 50.)\n",
    "    ax.set_ylim(-1.4, 0.4)\n",
    "\n",
    "    ax.set_xlabel(r'$r$')\n",
    "    ax.set_ylabel(r'$\\Phi$')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ttcKJXLPCq56"
   },
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs\n",
    "n_epochs = 4096\n",
    "\n",
    "# How much to weight Laplacian in loss function\n",
    "lam = torch.Tensor([2.0])  # Penalty for negative matter densities\n",
    "mu = torch.Tensor([0.])   # Penalty for positive matter densities\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    lam = lam.cuda()\n",
    "    mu = mu.cuda()\n",
    "    \n",
    "# Optimizer\n",
    "from flow_torch import RAdam\n",
    "n_steps = n_epochs * (n_data // batch_size)\n",
    "print(f'{n_steps} steps planned.')\n",
    "\n",
    "opt = RAdam(phi_param, lr=5e-2)\n",
    "decayRate = 0.9992\n",
    "lr_schedule = torch.optim.lr_scheduler.ExponentialLR(optimizer=opt, gamma=decayRate)\n",
    " \n",
    "# Optimze\n",
    "loss_history = []\n",
    "t0 = time()\n",
    "i = 0\n",
    "\n",
    "x_plot,v_plot = plummer_sphere.sample_df(1024)\n",
    "q_plot = torch.Tensor(x_plot.astype('f4'))\n",
    "if torch.cuda.is_available():\n",
    "    q_plot = q_plot.cuda()\n",
    "        \n",
    "for e in range(n_epochs):\n",
    "    for batch_idx, b in enumerate(train_loader):\n",
    "        q_b, p_b, df_dq_b, df_dp_b = [\n",
    "            torch.squeeze(x) for x in torch.split(b, 1, dim=1)\n",
    "        ]\n",
    "        \n",
    "        # Calculate the loss and its gradients w.r.t. the parameters\n",
    "        loss, dloss_dparam = potential_torch.get_phi_loss_gradients(\n",
    "            phi_model, phi_param,\n",
    "            q_b, p_b,\n",
    "            df_dq=df_dq_b,\n",
    "            df_dp=df_dp_b,\n",
    "            lam=lam,\n",
    "            mu=mu,\n",
    "            weight_samples=False\n",
    "        )\n",
    "\n",
    "        phi_model.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            lr_schedule.step()\n",
    "            \n",
    "        # Logging\n",
    "        loss_history.append(loss.detach().cpu())\n",
    "\n",
    "        if (i % 128 == 0) or (i == n_steps - 1):\n",
    "            loss_avg = np.mean(loss_history[-128:])\n",
    "            lr = opt.param_groups[0]['lr'] \n",
    "            print(\n",
    "                f'Step {i+1} of {n_steps} : '\n",
    "                f'<loss> = {loss_avg:.5g} '\n",
    "                f'lr = {lr:.5g}'\n",
    "            )\n",
    "\n",
    "t1 = time()\n",
    "print(f'Elapsed time: {t1-t0:.1f} s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models\n",
    "torch.save(phi_model.state_dict(), 'plummer_sphere_phi.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "yTHXpnIMOx-M",
    "outputId": "ac5a817e-89e5-47eb-ce40-8fb9a5029a56"
   },
   "outputs": [],
   "source": [
    "w = np.kaiser(250,5)\n",
    "w /= np.sum(w)\n",
    "loss_conv = np.hstack([\n",
    "    np.repeat(loss_history[0], 125),\n",
    "    np.array(loss_history),\n",
    "    np.array(loss_history)[-125:][::-1]\n",
    "])\n",
    "loss_conv = np.convolve(loss_conv, w, mode='valid')\n",
    "plt.semilogy(np.arange(len(loss_history)), loss_history, alpha=0.1)\n",
    "plt.semilogy(np.arange(len(loss_conv)), loss_conv)\n",
    "plt.grid('on', which='major', alpha=0.25)\n",
    "plt.grid('on', which='minor', alpha=0.05)\n",
    "plt.ylabel('loss (smoothed)')\n",
    "plt.xlabel('training step')\n",
    "plt.savefig('loss.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4rDUj99L0QD-"
   },
   "outputs": [],
   "source": [
    "def plot_phi(phi_nn, q):\n",
    "    fig,(ax1,ax2,ax3) = plt.subplots(1,3, figsize=(13,4))#, subplot_kw=dict(aspect='auto'))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        q = q.cuda()\n",
    "        \n",
    "    # phi vs. r\n",
    "    r = torch.sqrt(torch.sum(q**2, axis=1))\n",
    "            \n",
    "    phi_r = phi_nn(q).detach().cpu().numpy()\n",
    "    phi_theory_r = plummer_sphere.phi(r.detach().cpu().numpy())\n",
    "    phi_0 = np.median(phi_r - phi_theory_r)\n",
    "    \n",
    "    r_max = 8.\n",
    "\n",
    "    r_range = np.linspace(0.01, r_max, 100)\n",
    "    phi_theory_r = plummer_sphere.phi(r_range)\n",
    "    ax1.scatter(r.detach().cpu(), phi_r-phi_0, alpha=0.05, s=3)\n",
    "    ax1.plot(r_range, phi_theory_r, c='g', alpha=0.5)\n",
    "    ax1.set_xlabel(r'$r$')\n",
    "    ax1.set_ylabel(r'$\\phi$')\n",
    "    ax1.set_xlim(0., r_max)\n",
    "    ax1.set_ylim(-1.1, 0.2)\n",
    "\n",
    "    # phi in (x,y)-plane\n",
    "    x = np.linspace(-r_max, r_max, 50)\n",
    "    y = np.linspace(-r_max, r_max, 50)\n",
    "    xlim = (x[0], x[-1])\n",
    "    ylim = (y[0], y[-1])\n",
    "    x,y = np.meshgrid(x, y)\n",
    "    s = x.shape\n",
    "    x.shape = (x.size,)\n",
    "    y.shape = (y.size,)\n",
    "    xyz = np.stack([x,y,np.zeros_like(x)], axis=1)\n",
    "    q_grid = Variable(torch.Tensor(xyz.astype('f4')), requires_grad=True)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        q_grid = q_grid.cuda()\n",
    "        \n",
    "    phi_img = phi_nn(q_grid).detach().cpu().numpy()\n",
    "    phi_img = np.reshape(phi_img, s)\n",
    "    ax2.imshow(phi_img, extent=xlim+ylim)\n",
    "    ax2.set_xlabel(r'$x$')\n",
    "    ax2.set_ylabel(r'$y$')\n",
    "    ax2.set_title(r'$\\phi$')\n",
    "\n",
    "    # log(rho) in (x,y)-plane\n",
    "    p_grid = Variable(torch.Tensor(np.random.normal(size=q_grid.shape)), requires_grad=True)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        p_grid = p_grid.cuda()\n",
    "        \n",
    "    _,rho_img = potential_torch.calc_phi_derivatives(phi_nn, q_grid)\n",
    "    rho_img = np.reshape(rho_img.detach().cpu().numpy(), s)\n",
    "    ax3.imshow(np.log(rho_img), extent=xlim+ylim)\n",
    "    ax3.set_xlabel(r'$x$')\n",
    "    ax3.set_yticklabels([])\n",
    "    # ax3.set_ylabel(r'$y$')\n",
    "    ax3.set_title(r'$\\ln \\, \\rho$')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "colab_type": "code",
    "id": "PJIg3d9pBeqo",
    "outputId": "8ff124ef-855e-4f4c-9a93-765cc34a0257"
   },
   "outputs": [],
   "source": [
    "fig = plot_phi(phi_model, q[:1000])\n",
    "fig.savefig('phi_rho.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 908
    },
    "colab_type": "code",
    "id": "9KJYkRSYTDyw",
    "outputId": "5f12e26d-6d8a-4105-f958-de0edc1de1d6"
   },
   "outputs": [],
   "source": [
    "!ffmpeg -y -r 10 -pattern_type glob -i 'phi_training_*.png' -c:v libx264 -vf fps=10 -pix_fmt yuv420p phi_training.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeurIPS Flow Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neurips_flow_figure():\n",
    "    r_lim = (0., 5.)\n",
    "    v_lim = (0., 1.5)\n",
    "    bins = (50, 50)\n",
    "\n",
    "    r = np.linspace(r_lim[0], r_lim[1], 2*bins[0]+1)\n",
    "    v = np.linspace(v_lim[0], v_lim[1], 2*bins[1]+1)\n",
    "\n",
    "    r = 0.5 * (r[:-1] + r[1:])\n",
    "    v = 0.5 * (v[:-1] + v[1:])\n",
    "\n",
    "    rr,vv = np.meshgrid(r, v)\n",
    "\n",
    "    psi = 1. / np.sqrt(1+rr**2)\n",
    "    E = psi - vv**2 / 2\n",
    "    df = np.clip(E, 0., np.inf)**(7/2)\n",
    "    A = 24 * np.sqrt(2.) / (7 * np.pi**3)\n",
    "\n",
    "    n = A * (4*np.pi)**2 * rr**2 * vv**2 * df\n",
    "\n",
    "    # Downsample by a factor of 2\n",
    "    n = 0.5 * (n[:-1:2] + n[1::2])\n",
    "    n = 0.5 * (n[:,:-1:2] + n[:,1::2])\n",
    "    r = 0.5 * (r[:-1:2] + r[1::2])\n",
    "    v = 0.5 * (v[:-1:2] + v[1::2])\n",
    "\n",
    "    fig,ax_arr = plt.subplots(\n",
    "        1,4,\n",
    "        figsize=(7,2.5),\n",
    "        gridspec_kw=dict(width_ratios=[1,1,1,0.1]),\n",
    "        dpi=200\n",
    "    )\n",
    "    fig.subplots_adjust(\n",
    "        left=0.07,\n",
    "        right=0.90,\n",
    "        bottom=0.17,\n",
    "        top=0.90,\n",
    "        wspace=0.1\n",
    "    )\n",
    "\n",
    "    ax_ideal,ax_flow,ax_diff,cax_diff = ax_arr.flat\n",
    "\n",
    "    # Ideal distribution\n",
    "    dr = r[1] - r[0]\n",
    "    dv = v[1] - v[0]\n",
    "    N = np.sum(n) * dr * dv\n",
    "    print(f'\\int f(x,v) d^3x d^3v = {N:.5f}')\n",
    "\n",
    "    # vmax = np.nanmax(N)\n",
    "    im = ax_ideal.imshow(\n",
    "        n,\n",
    "        extent=r_lim+v_lim,\n",
    "        origin='lower',\n",
    "        aspect='auto',\n",
    "        interpolation='nearest',\n",
    "        # rasterized=True\n",
    "    )\n",
    "\n",
    "    # 2D histogram of samples\n",
    "    #n_samples = 1024*1024*4 # Increase this number to increase quality of figure\n",
    "    n_samples = 256*256*4 # Increase this number to increase quality of figure\n",
    "    \n",
    "    # Modify this to sample from your flow\n",
    "    eta_samp = []\n",
    "    batch_size = 4650\n",
    "    for batch in range(n_samples // batch_size):\n",
    "        \n",
    "        n_dim = 6\n",
    "        n_units = 4\n",
    "        fname = f'../plummer_flow/plummer_flow_{batch:02d}.pth'\n",
    "        state_dict = torch.load(fname)\n",
    "        flow = flow_torch.NormalizingFlow(n_dim, n_units)\n",
    "        flow.load_state_dict(state_dict)\n",
    "\n",
    "        x_sample = flow.dist.sample(sample_shape=[batch_size])\n",
    "        y_sample, _ = flow.backward(x_sample)\n",
    "        y_sample = y_sample.detach().numpy()\n",
    "        eta_samp.append(y_sample)\n",
    "    eta_samp = np.concatenate(eta_samp, axis=0)\n",
    "\n",
    "    x_samp,v_samp = np.split(eta_samp, 2, axis=1)\n",
    "    r_samp = np.sqrt(np.sum(x_samp**2, axis=1))\n",
    "    v_samp = np.sqrt(np.sum(v_samp**2, axis=1))\n",
    "\n",
    "    n_samp,_,_,_ = ax_flow.hist2d(\n",
    "        r_samp,\n",
    "        v_samp,\n",
    "        bins=bins,\n",
    "        range=[r_lim,v_lim],\n",
    "        density=False,\n",
    "        rasterized=True\n",
    "    )\n",
    "\n",
    "    # Residuals (samples - ideal)\n",
    "    n_0 = n*dr*dv * n_samples\n",
    "    # img = (n_samp.T - n_0) / n_0\n",
    "    img = np.log10(n_samp.T) - np.log10(n_0)\n",
    "    im_diff = ax_diff.imshow(\n",
    "        img,\n",
    "        extent=r_lim+v_lim,\n",
    "        vmax=0.4,\n",
    "        vmin=-0.4,\n",
    "        origin='lower',\n",
    "        aspect='auto',\n",
    "        cmap='coolwarm_r',\n",
    "        interpolation='nearest',\n",
    "        # rasterized=True\n",
    "    )\n",
    "\n",
    "    # Zero-energy line\n",
    "    for a,c in ((ax_ideal,'w'),(ax_flow,'w'),(ax_diff,'k')):\n",
    "        a.plot(r, np.sqrt(2.) * (1+r**2)**(-1/4), c=c)\n",
    "        a.set_xlabel(r'$r$')\n",
    "        a.text(\n",
    "            0.95, 0.95, r'$E > 0$',\n",
    "            ha='right', va='top',\n",
    "            fontsize=16, c=c,\n",
    "            transform=a.transAxes\n",
    "        )\n",
    "        a.yaxis.set_major_locator(MultipleLocator(2.))\n",
    "        a.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "        a.yaxis.set_major_locator(MultipleLocator(0.5))\n",
    "        a.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "    \n",
    "    # Colorbars\n",
    "    # cb = fig.colorbar(im, cax=cax, label=r'$N$')\n",
    "    cb_diff = fig.colorbar(\n",
    "        im_diff,\n",
    "        cax=cax_diff,\n",
    "        label=r'$\\log_{10} \\left( f_{\\varphi^{\\ast}} / f \\right)$',\n",
    "        extend='both'\n",
    "    )\n",
    "\n",
    "    # Axes labels\n",
    "    ax_flow.set_yticklabels([])\n",
    "    ax_diff.set_yticklabels([])\n",
    "    ax_ideal.set_ylabel(r'$v$')\n",
    "\n",
    "    # Labels\n",
    "    ax_ideal.set_title(r'$\\mathrm{Ideal\\ DF}$')\n",
    "    ax_flow.set_title(r'$\\mathrm{Flow}$')\n",
    "    # ax_diff.set_title(r'$\\left( \\mathrm{Flow} - \\mathrm{Ideal} \\right) \\, / \\, \\mathrm{Ideal}$')\n",
    "    ax_diff.set_title(r'$\\log_{10} \\left( \\mathrm{Flow} / \\mathrm{Ideal} \\right)$')\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig = create_neurips_flow_figure()\n",
    "plt.savefig('flow_vs_ideal.pdf', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't run past here.\n",
    "\n",
    "# NeurIPS Potential Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_nn = potential_torch.PhiNN(n_dim=3, n_hidden=3, n_features=128)\n",
    "\n",
    "# load trained model\n",
    "state_dict = torch.load('plummer_sphere_phi_lambda=2_mu=0.pth', map_location=torch.device('cpu'))\n",
    "phi_nn.load_state_dict(state_dict)\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    phi_nn = phi_nn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phi(phi_nn, r_max=13., x_max=5., n_samples=1024, grid_size=51):\n",
    "    plummer_sphere = toy_systems.PlummerSphere()\n",
    "    q,_ = plummer_sphere.sample_df(n_samples)\n",
    "    q = Variable(torch.Tensor(q.astype('f4')), requires_grad=True)\n",
    "\n",
    "    fig = plt.figure(figsize=(9,2.5), dpi=200)\n",
    "    gs_left = GridSpec(1,2, left=0.07, right=0.50, wspace=0.28)\n",
    "    gs_right = GridSpec(1,2, left=0.55, right=0.99, wspace=0.05)\n",
    "    fig.subplots_adjust(bottom=0.17, top=0.90)\n",
    "\n",
    "    # add plots to the nested structure\n",
    "    ax_phisc = fig.add_subplot(gs_left[0,0])\n",
    "    ax_rhosc = fig.add_subplot(gs_left[0,1])\n",
    "    ax2 = fig.add_subplot(gs_right[0,0])\n",
    "    ax3 = fig.add_subplot(gs_right[0,1])\n",
    "\n",
    "    # phi vs. r\n",
    "    r = torch.sqrt(torch.sum(q**2, axis=1))\n",
    "    phi_r = phi_nn(q).detach().numpy()\n",
    "    phi_theory_r = plummer_sphere.phi(r.detach().numpy())\n",
    "    phi_0 = np.median(phi_r - phi_theory_r)\n",
    "\n",
    "    # rho vs. r\n",
    "    # rho_theory_r = plummer_sphere.rho(r.numpy())\n",
    "    _, d2phi_dq2 = potential_torch.calc_phi_derivatives(phi_nn, q)\n",
    "    rho_r = d2phi_dq2.detach().numpy() / (4.*np.pi)\n",
    "\n",
    "    r_range = np.logspace(-1.1, np.log10(r_max), 100, base=10.)\n",
    "    # r_range = np.linspace(0.0, r_max, 100)\n",
    "    phi_theory_r = plummer_sphere.phi(r_range)\n",
    "    \n",
    "    ax_phisc.scatter(\n",
    "        r.detach().numpy(), phi_r-phi_0,\n",
    "        alpha=0.08,\n",
    "        s=3,\n",
    "        label=r'$\\mathrm{Approximation}$'\n",
    "    )\n",
    "    ax_phisc.semilogx(\n",
    "        r_range,\n",
    "        phi_theory_r,\n",
    "        c='g',\n",
    "        alpha=0.5,\n",
    "        label=r'$\\mathrm{Theory}$'\n",
    "    )\n",
    "    ax_phisc.set_xlabel(r'$r$', labelpad=-1)\n",
    "    ax_phisc.set_ylabel(r'$\\Phi$', labelpad=0)\n",
    "    ax_phisc.set_xticks([0.01, 0.1, 1., 10.])\n",
    "    ax_phisc.set_xlim(10**(-1.1), r_max)\n",
    "    ax_phisc.set_ylim(-1.1, 0.1)\n",
    "    ax_phisc.yaxis.set_major_locator(MultipleLocator(0.5))\n",
    "    ax_phisc.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "    leg = ax_phisc.legend(loc='upper left', fontsize=8)\n",
    "    for lh in leg.legendHandles:\n",
    "        lh.set_alpha(1)\n",
    "    \n",
    "    rho_theory_r = plummer_sphere.rho(r_range)\n",
    "    ax_rhosc.scatter(\n",
    "        r.detach().numpy(), rho_r,\n",
    "        alpha=0.08,\n",
    "        s=3,\n",
    "        label=r'$\\mathrm{Approximation}$'\n",
    "    )\n",
    "    ax_rhosc.semilogx(\n",
    "        r_range,\n",
    "        rho_theory_r,\n",
    "        c='g',\n",
    "        alpha=0.5,\n",
    "        label=r'$\\mathrm{Theory}$'\n",
    "    )\n",
    "    ax_rhosc.set_xlabel(r'$r$', labelpad=-1)\n",
    "    ax_rhosc.set_ylabel(r'$\\rho$', labelpad=1)\n",
    "    ax_rhosc.set_xticks([0.01, 0.1, 1., 10.])\n",
    "    ax_rhosc.set_xlim(10**(-1.1), r_max)\n",
    "    ax_rhosc.set_ylim(-0.1*rho_theory_r[0], 1.1*rho_theory_r[0])\n",
    "    ax_rhosc.axhline(0., c='k', alpha=0.5)\n",
    "    ax_rhosc.yaxis.set_major_locator(MultipleLocator(0.1))\n",
    "    ax_rhosc.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "    leg = ax_rhosc.legend(loc='upper right', fontsize=8)\n",
    "    for lh in leg.legendHandles:\n",
    "        lh.set_alpha(1)\n",
    "\n",
    "    # phi in (x,y)-plane\n",
    "    x = np.linspace(-x_max, x_max, grid_size)\n",
    "    y = np.linspace(-x_max, x_max, grid_size)\n",
    "    xlim = (x[0], x[-1])\n",
    "    ylim = (y[0], y[-1])\n",
    "    x,y = np.meshgrid(x, y)\n",
    "    s = x.shape\n",
    "    x.shape = (x.size,)\n",
    "    y.shape = (y.size,)\n",
    "    xyz = np.stack([x,y,np.zeros_like(x)], axis=1)\n",
    "    q_grid = Variable(torch.Tensor(xyz.astype('f4')), requires_grad=True)\n",
    "    phi_img = phi_nn(q_grid).detach().numpy()\n",
    "    phi_img = np.reshape(phi_img, s)\n",
    "    ax2.imshow(phi_img, extent=xlim+ylim)\n",
    "    ax2.set_xlabel(r'$x$', labelpad=-1)\n",
    "    ax2.set_ylabel(r'$y$', labelpad=-2)\n",
    "    ax2.set_title(r'$\\Phi$')\n",
    "\n",
    "    # log(rho) in (x,y)-plane\n",
    "    p_grid = Variable(torch.Tensor(np.random.normal(q_grid.shape)), requires_grad=True)\n",
    "    _,rho_img = potential_torch.calc_phi_derivatives(phi_nn, q_grid)\n",
    "    rho_img = np.reshape(rho_img.detach().numpy(), s)\n",
    "    ax3.imshow(np.log(rho_img), extent=xlim+ylim)\n",
    "    # rho_img[rho_img < 0] = np.nan\n",
    "    # ax3.imshow(np.sqrt(rho_img), extent=xlim+ylim)\n",
    "    ax3.set_xlabel(r'$x$', labelpad=-1)\n",
    "    ax3.set_yticklabels([])\n",
    "    ax3.set_title(r'$\\ln \\rho$')\n",
    "\n",
    "    for a in (ax2,ax3):\n",
    "        a.xaxis.set_major_locator(MultipleLocator(4.))\n",
    "        a.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "        a.yaxis.set_major_locator(MultipleLocator(4.))\n",
    "        a.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig = plot_phi(phi_nn, n_samples=1024, grid_size=51, r_max=13.)\n",
    "fig.savefig('potential_vs_ideal.png', dpi=200)\n",
    "fig.savefig('potential_vs_ideal.pdf', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_phi(phi_nn, r_max=13., x_max=5., n_samples=1024, grid_size=51):\n",
    "    plummer_sphere = toy_systems.PlummerSphere()\n",
    "    q,_ = plummer_sphere.sample_df(n_samples)\n",
    "    q = torch.Tensor(q.astype('f4'))\n",
    "\n",
    "    fig = plt.figure(figsize=(7,2.5), dpi=200)\n",
    "    gs_left = GridSpec(1,1, left=0.09, right=0.35)\n",
    "    gs_right = GridSpec(1,2, left=0.42, right=0.99, wspace=0.07)\n",
    "    fig.subplots_adjust(bottom=0.17, top=0.90)\n",
    "\n",
    "    # add plots to the nested structure\n",
    "    ax1 = fig.add_subplot(gs_left[0,0])\n",
    "    ax2 = fig.add_subplot(gs_right[0,0])\n",
    "    ax3 = fig.add_subplot(gs_right[0,1])\n",
    "\n",
    "    # phi vs. r\n",
    "    r = torch.sqrt(torch.sum(q**2, axis=1))\n",
    "    phi_r = phi_nn(q).detach().numpy()\n",
    "    phi_theory_r = plummer_sphere.phi(r.detach().numpy())\n",
    "    phi_0 = np.median(phi_r - phi_theory_r)\n",
    "\n",
    "    r_range = np.logspace(-1.1, np.log10(r_max), 100, base=10.)\n",
    "    # r_range = np.linspace(0.0, r_max, 100)\n",
    "    phi_theory_r = plummer_sphere.phi(r_range)\n",
    "    # ax1.plot(\n",
    "    ax1.scatter(\n",
    "        r, phi_r-phi_0,\n",
    "        alpha=0.08,\n",
    "        s=3,\n",
    "        label=r'$\\mathrm{Approximation}$'\n",
    "    )\n",
    "    ax1.semilogx(\n",
    "        r_range,\n",
    "        phi_theory_r,\n",
    "        c='g',\n",
    "        alpha=0.5,\n",
    "        label=r'$\\mathrm{Theory}$'\n",
    "    )\n",
    "    ax1.set_xlabel(r'$r$')\n",
    "    ax1.set_ylabel(r'$\\Phi$')\n",
    "    ax1.set_xlim(10**(-1.1), r_max)\n",
    "    ax1.set_xlim(0., r_max)\n",
    "    ax1.set_ylim(-1.1, 0.1)\n",
    "    # ax1.xaxis.set_xticks([0.01, 0.1, 0., 10.])\n",
    "    # ax1.xaxis.set_major_locator(MultipleLocator(4.))\n",
    "    # ax1.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "    ax1.yaxis.set_major_locator(MultipleLocator(0.5))\n",
    "    ax1.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "    leg = ax1.legend(loc='upper left', fontsize=8)\n",
    "    for lh in leg.legendHandles:\n",
    "        lh.set_alpha(1)\n",
    "\n",
    "    # phi in (x,y)-plane\n",
    "    x = np.linspace(-x_max, x_max, grid_size)\n",
    "    y = np.linspace(-x_max, x_max, grid_size)\n",
    "    xlim = (x[0], x[-1])\n",
    "    ylim = (y[0], y[-1])\n",
    "    x,y = np.meshgrid(x, y)\n",
    "    s = x.shape\n",
    "    x.shape = (x.size,)\n",
    "    y.shape = (y.size,)\n",
    "    xyz = np.stack([x,y,np.zeros_like(x)], axis=1)\n",
    "    q_grid = Variable(torch.Tensor(xyz.astype('f4')), requires_grad=True)\n",
    "    phi_img = phi_nn(q_grid).detach().numpy()\n",
    "    phi_img = np.reshape(phi_img, s)\n",
    "    ax2.imshow(phi_img, extent=xlim+ylim)\n",
    "    ax2.set_xlabel(r'$x$')\n",
    "    ax2.set_ylabel(r'$y$', labelpad=-2)\n",
    "    ax2.set_title(r'$\\Phi$')\n",
    "\n",
    "    # log(rho) in (x,y)-plane\n",
    "    p_grid = Variable(torch.Tensor(np.random.normal(q_grid.shape)), requires_grad=True)\n",
    "    _,rho_img = potential_torch.calc_phi_derivatives(phi_nn, q_grid)\n",
    "    rho_img = np.reshape(rho_img.detach().numpy(), s)\n",
    "    ax3.imshow(np.log(rho_img), extent=xlim+ylim)\n",
    "    ax3.set_xlabel(r'$x$')\n",
    "    ax3.set_yticklabels([])\n",
    "    # ax3.set_ylabel(r'$y$')\n",
    "    ax3.set_title(r'$\\ln \\rho$')\n",
    "\n",
    "    for a in (ax2,ax3):\n",
    "        a.xaxis.set_major_locator(MultipleLocator(4.))\n",
    "        a.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "        a.yaxis.set_major_locator(MultipleLocator(4.))\n",
    "        a.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig = plot_phi(phi_nn, n_samples=1024, grid_size=51, r_max=13.)\n",
    "fig.savefig('potential_vs_ideal.pdf', dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make video for presentations.\n",
    "\n",
    "> Train a specific flow that save each time step for the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define plot properties\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from matplotlib import rcParams\n",
    "from matplotlib import rc\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def rgb(r,g,b):\n",
    "    return (float(r)/256.,float(g)/256.,float(b)/256.)\n",
    "\n",
    "cb2 = [rgb(31,120,180), rgb(255,127,0), rgb(51,160,44), rgb(227,26,28), \\\n",
    "       rgb(10,10,10), rgb(253,191,111), rgb(178,223,138), rgb(251,154,153)]\n",
    "\n",
    "\n",
    "#===================================================================================\n",
    "def create_neurips_flow_figure(ind):\n",
    "    r_lim = (0., 5.)\n",
    "    v_lim = (0., 1.5)\n",
    "    bins = (100, 100)\n",
    "\n",
    "    r = np.linspace(r_lim[0], r_lim[1], 2*bins[0]+1)\n",
    "    v = np.linspace(v_lim[0], v_lim[1], 2*bins[1]+1)\n",
    "\n",
    "    r = 0.5 * (r[:-1] + r[1:])\n",
    "    v = 0.5 * (v[:-1] + v[1:])\n",
    "    r = 0.5 * (r[:-1:2] + r[1::2])\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "    fig,ax_arr = plt.subplots(\n",
    "        1,2,\n",
    "        figsize=(7,3.5),\n",
    "        gridspec_kw=dict(width_ratios=[1,1]),\n",
    "        dpi=200\n",
    "    )\n",
    "    fig.subplots_adjust(\n",
    "        left=0.07,\n",
    "        right=0.90,\n",
    "        bottom=0.17,\n",
    "        top=0.90,\n",
    "        wspace=0.1\n",
    "    )\n",
    "\n",
    "    ax_ideal,ax_flow = ax_arr.flat\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "    # 2D histogram of samples\n",
    "    #n_samples = 1024*1024*4 # Increase this number to increase quality of figure\n",
    "    n_samples = 256*256*4 # Increase this number to increase quality of figure\n",
    "    \n",
    "    # Modify this to sample from your flow\n",
    "    eta_samp = []\n",
    "    batch_size = 4650\n",
    "    for batch in range(n_samples // batch_size):\n",
    "        n_dim = 6\n",
    "        n_units = 4\n",
    "        fname = f'plummer_flow_video_' + str(ind) + '.pth'\n",
    "        state_dict = torch.load(fname)\n",
    "        flow = flow_torch.NormalizingFlow(n_dim, n_units)\n",
    "        flow.load_state_dict(state_dict)\n",
    "\n",
    "        x_sample = flow.dist.sample(sample_shape=[batch_size])\n",
    "        y_sample, _ = flow.backward(x_sample)\n",
    "        y_sample = y_sample.detach().numpy()\n",
    "        eta_samp.append(y_sample)\n",
    "        \n",
    "    eta_samp = np.concatenate(eta_samp, axis=0)\n",
    "\n",
    "    x_samp,v_samp = np.split(eta_samp, 2, axis=1)\n",
    "    r_samp = np.sqrt(np.sum(x_samp**2, axis=1))\n",
    "    v_samp = np.sqrt(np.sum(v_samp**2, axis=1))\n",
    "    print(r_samp.shape)\n",
    "    \n",
    "    n_samp,_,_,_ = ax_flow.hist2d(\n",
    "        r_samp,\n",
    "        v_samp,\n",
    "        bins=bins,\n",
    "        range=[r_lim,v_lim],\n",
    "        density=False,\n",
    "        rasterized=True,\n",
    "        cmap='inferno_r'\n",
    "    )\n",
    "\n",
    "#----------------------------------------------------------------------------------\n",
    "    # the scatter plot\n",
    "    ax_ideal.scatter(\n",
    "        r_samp_template[::500],\n",
    "        v_samp_template[::500],\n",
    "        color=cb2[0], s=5\n",
    "    )\n",
    "    ax_ideal.set_xlim(r_lim)\n",
    "    ax_ideal.set_ylim(v_lim)\n",
    "    \n",
    "#----------------------------------------------------------------------------------\n",
    "    # Zero-energy line\n",
    "    for a,c in ((ax_ideal,'k'),(ax_flow,'k')):\n",
    "        a.plot(r, np.sqrt(2.) * (1+r**2)**(-1/4), c=c)\n",
    "        a.set_xlabel(r'$r$')\n",
    "        a.yaxis.set_major_locator(MultipleLocator(2.))\n",
    "        a.xaxis.set_minor_locator(AutoMinorLocator())\n",
    "        a.yaxis.set_major_locator(MultipleLocator(0.5))\n",
    "        a.yaxis.set_minor_locator(AutoMinorLocator())\n",
    "    \n",
    "#----------------------------------------------------------------------------------\n",
    "    # Axes labels\n",
    "    ax_flow.set_yticklabels([])\n",
    "    ax_ideal.set_ylabel(r'$v$')\n",
    "\n",
    "    # Labels\n",
    "    ax_ideal.set_title(r'$\\mathrm{Ideal\\ DF}$')\n",
    "    ax_flow.set_title(r'$\\mathrm{Flow}$')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "#===================================================================================\n",
    "# fixed 2D histogram of samples\n",
    "n_samples = 256*256*4 # Increase this number to increase quality of figure\n",
    "    \n",
    "# Modify this to sample from your flow\n",
    "eta_samp = []\n",
    "batch_size = 4650\n",
    "for batch in range(n_samples // batch_size):\n",
    "    n_dim = 6\n",
    "    n_units = 4\n",
    "    fname = f'../plummer_flow/plummer_flow_{batch:02d}.pth'\n",
    "    state_dict = torch.load(fname)\n",
    "    flow = flow_torch.NormalizingFlow(n_dim, n_units)\n",
    "    flow.load_state_dict(state_dict)\n",
    "\n",
    "    x_sample = flow.dist.sample(sample_shape=[batch_size])\n",
    "    y_sample, _ = flow.backward(x_sample)\n",
    "    y_sample = y_sample.detach().numpy()\n",
    "    eta_samp.append(y_sample)\n",
    "\n",
    "eta_samp = np.concatenate(eta_samp, axis=0)\n",
    "\n",
    "x_samp,v_samp = np.split(eta_samp, 2, axis=1)\n",
    "r_samp_template = np.sqrt(np.sum(x_samp**2, axis=1))\n",
    "v_samp_template = np.sqrt(np.sum(v_samp**2, axis=1))\n",
    "\n",
    "\n",
    "#===================================================================================\n",
    "for ind in range(100):\n",
    "    fig = create_neurips_flow_figure(ind)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'flow_vs_ideal_{ind:02d}.png', dpi=200)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -y -r 10 -pattern_type glob -i 'flow_vs_ideal_*.png' -c:v libx264 -vf fps=3 -pix_fmt yuv420p flow_training.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Deep Potential - Plummer Sphere",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
